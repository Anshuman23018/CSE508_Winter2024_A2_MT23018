{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3fcf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\anshu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anshu\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/71F3npeHUDL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/71wHUWncMGL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/71B8OOE5N8L._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/81SX3oAWbNL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/718niQ1GEwL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/61OboZT-kcL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/710a2Pyh5lL._SY88.jpg. Removing entry.\n",
      "UnidentifiedImageError: cannot identify image file from URL https://images-na.ssl-images-amazon.com/images/I/816NMd0LexL._SY88.jpg. Removing entry.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchvision import models, transforms\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"A2_Data.csv\")\n",
    "\n",
    "# Create a new DataFrame to hold the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Iterate through each row in the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the ID and Review Text from the current row\n",
    "    id_value = row['ID']\n",
    "    review_text = row['Review Text']\n",
    "\n",
    "    # Split the Image links into separate rows\n",
    "    for image_link in eval(row['Image']):  # Use eval to convert string representation of list to an actual list\n",
    "        expanded_row = {\n",
    "            'ID': id_value,\n",
    "            'Image': image_link,\n",
    "            'Review Text': review_text\n",
    "        }\n",
    "        expanded_rows.append(expanded_row)\n",
    "\n",
    "# Create a new DataFrame from the expanded rows\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Define image preprocessing\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust to ResNet input size (224x224)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to extract features from an image using ResNet\n",
    "def extract_image_features_resnet(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        img_t = image_transforms(img)\n",
    "        img_t = img_t.unsqueeze(0)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            features = resnet(img_t)\n",
    "        return features.cpu().numpy().flatten()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"RequestException for URL {url}: {e}\")\n",
    "    except UnidentifiedImageError:\n",
    "        print(f\"UnidentifiedImageError: cannot identify image file from URL {url}. Removing entry.\")\n",
    "        return None  # Return None to indicate the image could not be processed\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for URL {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "# List to store indices of rows to be removed\n",
    "rows_to_remove = []\n",
    "\n",
    "# Extract features using ResNet\n",
    "image_features_resnet = []\n",
    "\n",
    "for index, row in expanded_df.iterrows():\n",
    "    # Check if the 'Image' column has a valid URL\n",
    "    if pd.notna(row['Image']):\n",
    "        image_feature = extract_image_features_resnet(row['Image'])\n",
    "        if image_feature is not None:\n",
    "            image_features_resnet.append(image_feature)\n",
    "        else:\n",
    "            # If the image could not be processed, mark the row for removal\n",
    "            rows_to_remove.append(index)\n",
    "\n",
    "# Normalize the extracted features using ResNet\n",
    "#image_features_resnet = np.array(image_features_resnet)\n",
    "#mean_resnet = np.mean(image_features_resnet, axis=0)\n",
    "#std_resnet = np.std(image_features_resnet, axis=0)\n",
    "#normalized_features_resnet = (image_features_resnet - mean_resnet) / std_resnet\n",
    "\n",
    "# Save the normalized features\n",
    "#with open('normalized_features_resnet.pkl', 'wb') as file:\n",
    "    #pickle.dump(normalized_features_resnet, file)\n",
    "\n",
    "# Remove rows marked for removal\n",
    "#expanded_df = expanded_df.drop(rows_to_remove, axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows marked for removal\n",
    "cleaned_df = expanded_df.drop(rows_to_remove, inplace=False).reset_index(drop=True)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "#cleaned_df.to_csv(\"cleaned_A2_Data.csv\", index=False)\n",
    "\n",
    "# Save the results using ResNet\n",
    "with open('normalized_features_resnet.pkl', 'wb') as f:\n",
    "    pickle.dump(cleaned_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54eca16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640\n"
     ]
    }
   ],
   "source": [
    "print(len(image_features_resnet))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe06a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                              Image  \\\n",
      "0     3452  https://images-na.ssl-images-amazon.com/images...   \n",
      "1     1205  https://images-na.ssl-images-amazon.com/images...   \n",
      "2     1205  https://images-na.ssl-images-amazon.com/images...   \n",
      "3     1205  https://images-na.ssl-images-amazon.com/images...   \n",
      "4     1708  https://images-na.ssl-images-amazon.com/images...   \n",
      "...    ...                                                ...   \n",
      "1635  1882  https://images-na.ssl-images-amazon.com/images...   \n",
      "1636  1547  https://images-na.ssl-images-amazon.com/images...   \n",
      "1637  1547  https://images-na.ssl-images-amazon.com/images...   \n",
      "1638  1004  https://images-na.ssl-images-amazon.com/images...   \n",
      "1639  1306  https://images-na.ssl-images-amazon.com/images...   \n",
      "\n",
      "                                            Review Text  \\\n",
      "0     Loving these vintage springs on my vintage str...   \n",
      "1     Works great as a guitar bench mat. Not rugged ...   \n",
      "2     Works great as a guitar bench mat. Not rugged ...   \n",
      "3     Works great as a guitar bench mat. Not rugged ...   \n",
      "4     We use these for everything from our acoustic ...   \n",
      "...                                                 ...   \n",
      "1635  This is a great stereo reverb with plenty of c...   \n",
      "1636  I really like the simplicity of this bridge. I...   \n",
      "1637  I really like the simplicity of this bridge. I...   \n",
      "1638  Great Product, but there is no warranty in the...   \n",
      "1639  This product is good and is used in profession...   \n",
      "\n",
      "                                         Image features  \n",
      "0     [1.371263, -1.0438677, -0.7493192, -1.3473889,...  \n",
      "1     [-3.1160443, -1.976818, -0.43816474, -1.253221...  \n",
      "2     [-0.0317304, -1.9518839, -3.5011334, -3.022237...  \n",
      "3     [-2.7878191, -1.1015264, -1.8915395, -2.818510...  \n",
      "4     [-3.061731, -1.2005367, -0.62638843, -2.693416...  \n",
      "...                                                 ...  \n",
      "1635  [-3.6147218, -3.048591, -3.6973, -4.0487447, -...  \n",
      "1636  [-2.298172, -0.8628822, 0.21242723, -2.534635,...  \n",
      "1637  [-0.45014846, -0.0388508, -0.05530607, -0.8738...  \n",
      "1638  [-2.1133106, -1.762419, -1.5130885, -2.7888627...  \n",
      "1639  [-0.9160093, 1.7691718, -3.2258902, -3.8859973...  \n",
      "\n",
      "[1640 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming cleaned_df and image_features_resnet have the same length\n",
    "cleaned_df[\"Image features\"] = image_features_resnet\n",
    "\n",
    "# Display the DataFrame\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20630f2c",
   "metadata": {},
   "source": [
    "Text Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4b5cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anshu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF DataFrame:\n",
      "          love    vintag    spring     strat      good   tension     great  \\\n",
      "0     0.129524  0.520793  0.727539  0.200533  0.100905  0.327836  0.076447   \n",
      "1     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
      "2     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
      "3     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
      "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1635  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.156368   \n",
      "1636  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1637  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1638  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.099713   \n",
      "1639  0.000000  0.000000  0.000000  0.000000  0.151357  0.000000  0.000000   \n",
      "\n",
      "        stabil     float     bridg  ...  yngwie  neoclass  john  mayer  \\\n",
      "0     0.333637  0.339991  0.223960  ...     0.0       0.0   0.0    0.0   \n",
      "1     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "2     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "3     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "4     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "...        ...       ...       ...  ...     ...       ...   ...    ...   \n",
      "1635  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "1636  0.000000  0.000000  0.131741  ...     0.0       0.0   0.0    0.0   \n",
      "1637  0.000000  0.000000  0.131741  ...     0.0       0.0   0.0    0.0   \n",
      "1638  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "1639  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
      "\n",
      "      importantli  toneprint     biggi    accord  screenshot      piti  \n",
      "0             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "1             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "2             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "3             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "4             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "...           ...        ...       ...       ...         ...       ...  \n",
      "1635          0.0   0.336475  0.000000  0.000000    0.000000  0.000000  \n",
      "1636          0.0   0.000000  0.131555  0.000000    0.000000  0.000000  \n",
      "1637          0.0   0.000000  0.131555  0.000000    0.000000  0.000000  \n",
      "1638          0.0   0.000000  0.000000  0.321846    0.321846  0.321846  \n",
      "1639          0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
      "\n",
      "[1640 rows x 4124 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Lowercase conversion\n",
    "    text = text.lower()\n",
    "    # Remove URLs, hashtags, and mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Stopwords removal, stemming, and lemmatization\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(word)) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Assuming cleaned_df is your DataFrame\n",
    "text_data = cleaned_df['Review Text'].fillna('').tolist()\n",
    "\n",
    "# Preprocess text data\n",
    "tokenized_texts = [preprocess_text(text) for text in text_data]\n",
    "\n",
    "# Manual TF-IDF Calculation\n",
    "def compute_tf_idf(tokenized_docs):\n",
    "    # Count term frequencies using Counter\n",
    "    tf = [{word: count / len(doc) for word, count in Counter(doc).items()} for doc in tokenized_docs]\n",
    "\n",
    "    # Create a set of all unique words in all documents\n",
    "    all_words = set(word for doc in tokenized_docs for word in doc)\n",
    "\n",
    "    # Calculate document frequency (DF) using set operations\n",
    "    df = {word: sum(1 for doc in tokenized_docs if word in doc) for word in all_words}\n",
    "\n",
    "    # Calculate IDF (inverse document frequency)\n",
    "    idf = {word: math.log(len(tokenized_docs) / freq) for word, freq in df.items()}\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tf_idf = [{word: freq * idf[word] for word, freq in doc.items()} for doc in tf]\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf_scores_text = compute_tf_idf(tokenized_texts)\n",
    "\n",
    "# Specify paths for saving tokenized texts and TF-IDF scores\n",
    "tokenized_texts_path = 'tokenized_texts_text.pkl'\n",
    "tf_idf_scores_path = 'tf_idf_scores_manual_text.pkl'\n",
    "\n",
    "# Save tokenized texts\n",
    "with open(tokenized_texts_path, 'wb') as f:\n",
    "    pickle.dump(tokenized_texts, f)\n",
    "\n",
    "# Save TF-IDF scores\n",
    "with open(tf_idf_scores_path, 'wb') as f:\n",
    "    pickle.dump(tf_idf_scores_text, f)\n",
    "\n",
    "# Load tokenized texts\n",
    "with open('tokenized_texts_text.pkl', 'rb') as f:\n",
    "    tokenized_texts = pickle.load(f)\n",
    "\n",
    "# Load TF-IDF scores\n",
    "with open('tf_idf_scores_manual_text.pkl', 'rb') as f:\n",
    "    tf_idf_scores = pickle.load(f)\n",
    "\n",
    "# Create a DataFrame from TF-IDF scores\n",
    "tf_idf_pd = pd.DataFrame(tf_idf_scores)\n",
    "tf_idf_pd.fillna(0, inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"TF-IDF DataFrame:\")\n",
    "print(tf_idf_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c936994d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>love</th>\n",
       "      <th>vintag</th>\n",
       "      <th>spring</th>\n",
       "      <th>strat</th>\n",
       "      <th>good</th>\n",
       "      <th>tension</th>\n",
       "      <th>great</th>\n",
       "      <th>stabil</th>\n",
       "      <th>float</th>\n",
       "      <th>bridg</th>\n",
       "      <th>...</th>\n",
       "      <th>yngwie</th>\n",
       "      <th>neoclass</th>\n",
       "      <th>john</th>\n",
       "      <th>mayer</th>\n",
       "      <th>importantli</th>\n",
       "      <th>toneprint</th>\n",
       "      <th>biggi</th>\n",
       "      <th>accord</th>\n",
       "      <th>screenshot</th>\n",
       "      <th>piti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.129524</td>\n",
       "      <td>0.520793</td>\n",
       "      <td>0.727539</td>\n",
       "      <td>0.200533</td>\n",
       "      <td>0.100905</td>\n",
       "      <td>0.327836</td>\n",
       "      <td>0.076447</td>\n",
       "      <td>0.333637</td>\n",
       "      <td>0.339991</td>\n",
       "      <td>0.223960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321846</td>\n",
       "      <td>0.321846</td>\n",
       "      <td>0.321846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1640 rows × 4124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          love    vintag    spring     strat      good   tension     great  \\\n",
       "0     0.129524  0.520793  0.727539  0.200533  0.100905  0.327836  0.076447   \n",
       "1     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
       "2     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
       "3     0.000000  0.000000  0.000000  0.000000  0.065808  0.000000  0.049857   \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1635  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.156368   \n",
       "1636  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1637  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1638  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.099713   \n",
       "1639  0.000000  0.000000  0.000000  0.000000  0.151357  0.000000  0.000000   \n",
       "\n",
       "        stabil     float     bridg  ...  yngwie  neoclass  john  mayer  \\\n",
       "0     0.333637  0.339991  0.223960  ...     0.0       0.0   0.0    0.0   \n",
       "1     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "2     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "3     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "4     0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "...        ...       ...       ...  ...     ...       ...   ...    ...   \n",
       "1635  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "1636  0.000000  0.000000  0.131741  ...     0.0       0.0   0.0    0.0   \n",
       "1637  0.000000  0.000000  0.131741  ...     0.0       0.0   0.0    0.0   \n",
       "1638  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "1639  0.000000  0.000000  0.000000  ...     0.0       0.0   0.0    0.0   \n",
       "\n",
       "      importantli  toneprint     biggi    accord  screenshot      piti  \n",
       "0             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "1             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "2             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "3             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "4             0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "...           ...        ...       ...       ...         ...       ...  \n",
       "1635          0.0   0.336475  0.000000  0.000000    0.000000  0.000000  \n",
       "1636          0.0   0.000000  0.131555  0.000000    0.000000  0.000000  \n",
       "1637          0.0   0.000000  0.131555  0.000000    0.000000  0.000000  \n",
       "1638          0.0   0.000000  0.000000  0.321846    0.321846  0.321846  \n",
       "1639          0.0   0.000000  0.000000  0.000000    0.000000  0.000000  \n",
       "\n",
       "[1640 rows x 4124 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d2c8031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1640"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93554860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter image URL (or press Enter to finish): https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
      "REVIEW: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
      "----------------------------------------------------\n",
      "Top 3 similar images:\n",
      "1. Image URL: https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
      "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
      "Cosine score image: 1.0000001192092896\n",
      "Cosine score text: 0.07903665389491232\n",
      "Cosine score composite: 0.5395183865521009\n",
      "\n",
      "\n",
      "2. Image URL: https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg\n",
      "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
      "Cosine score image: 0.9115102291107178\n",
      "Cosine score text: 0.12994053496072275\n",
      "Cosine score composite: 0.5207253820357203\n",
      "\n",
      "\n",
      "3. Image URL: https://images-na.ssl-images-amazon.com/images/I/711XbxFTQpL._SY88.jpg\n",
      "Review: There is not any noise during use, and not heating with 8 pedals powered on running for 5 hours, the voltage of each channel keeps the same. Stable power supplying. With 7 X 9V 100mA +1 X 9V 500mA+1 X 12V 100mA+1 X 18V 100mA can work for at least 10 pedals easily. 10X50 cm cables are long enough to reach any part of the pedal board. Small size, and will not taking up too much pace. The Polarity Reversal Cable can help you cope with different pedals. Stable output and cost-effective. Provides 10 power outputs, it is able to work for a standard pedalboard. But the price is not so expensive as the Voodoo Lab or T-Rex, etc, you can easily have a power supply to work for your own pedals. I am quite satisfied with the price and stability overall it is very worth having. NO HUM! EXCELLENT POWER! GET ONE!\n",
      "Cosine score image: 0.8727928996086121\n",
      "Cosine score text: 0.12436565300296494\n",
      "Cosine score composite: 0.4985792763057885\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "Top 3 similar reviews:\n",
      "1. Image URL: https://images-na.ssl-images-amazon.com/images/I/71Md5ihUFLL._SY88.jpg\n",
      "Review: We use these for everything from our acoustic bass down to our ukuleles. I know there is a smaller model available for ukes, violins, etc.; we haven't yet ordered those, but these will work on smaller instruments if one doesn't extend the feet to their maximum width. They're gentle on the instruments, and the grippy material keeps them secure.\n",
      "\n",
      "The greatest benefit has been when writing music at the computer and needing to set a guitar down to use the keyboard/mouse - just easier for me than a hanging stand.\n",
      "\n",
      "We have several and gave one to a friend for Christmas as well. I've used mine on stage, and it folds up small enough to fit right in my gig bag.\n",
      "Cosine score image: 0.6928731799125671\n",
      "Cosine score text: 0.23185982835968613\n",
      "Cosine score composite: 0.4623665041361266\n",
      "\n",
      "\n",
      "2. Image URL: https://images-na.ssl-images-amazon.com/images/I/71w8aOdrTuL._SY88.jpg\n",
      "Review: I bought this bass to split time as my primary bass with my Dean Edge. This might be winning me over. The bass boost is outstanding. The active pickups really allow you to adjust to the sound you want. I recommend this for anyone. If you're a beginner  like I was not too long ago, it's an excellent bass to start with. If you're on tour and/or music is making you money, this bass will be beatiful on stage. The color is a bit darker than in the picture. But, all around, this is a great buy.\n",
      "Cosine score image: 0.6241376996040344\n",
      "Cosine score text: 0.19601729704762313\n",
      "Cosine score composite: 0.4100774983258288\n",
      "\n",
      "\n",
      "3. Image URL: https://images-na.ssl-images-amazon.com/images/I/71dCrR30OvL._SY88.jpg\n",
      "Review: Looking at these on a guitar when they don't have a single wrap on the post, I found it hard to believe the strings would even stay on but I have now installed two sets of these on two different guitars. One black set and the other is gold. The first guitar would not stay in tune so I tried these. They have been on for nearly a year now and they totally fixed my tuning problem. The second set I put on my brand new Gibson Les Paul only because the first ones worked so well. Super easy fast string changes with the auto trim feature, and no tuning issues at all. At 18:1 gear ratio they dial in a perfect tune and hold it. I prefer these over Grover or any other higher priced tuners available.\n",
      "I have added a few pictures. The new tuners covered the holes on my Les Paul so it looks great from the back. The Other is an Ibanez AR420. You can see the original mounting holes so I filled them with wood putty and will use model car paint to match to cover the spots.\n",
      "Cosine score image: 0.8388147950172424\n",
      "Cosine score text: 0.19374981200602875\n",
      "Cosine score composite: 0.5162823035116356\n",
      "\n",
      "\n",
      "----------------------------------------------------\n",
      "Top 3 similar composites:\n",
      "1. Image URL: https://images-na.ssl-images-amazon.com/images/I/71bztfqdg+L._SY88.jpg\n",
      "Review: I have been using Fender locking tuners for about five years on various strats and teles. Definitely helps with tuning stability and way faster to restring if there is a break.\n",
      "Cosine score image: 1.0000001192092896\n",
      "Cosine score text: 0.07903665389491232\n",
      "Cosine score composite: 0.5395183865521009\n",
      "\n",
      "\n",
      "2. Image URL: https://images-na.ssl-images-amazon.com/images/I/719-SDMiOoL._SY88.jpg\n",
      "Review: These locking tuners look great and keep tune.  Good quality materials and construction.  Excellent upgrade to any guitar.  I had to drill additions holes for installation.  If your neck already comes with pre-drilled holes, then they should drop right in, otherwise you will need to buy a guitar tuner pin drill jig, also available from Amazon.\n",
      "Cosine score image: 0.9115102291107178\n",
      "Cosine score text: 0.12994053496072275\n",
      "Cosine score composite: 0.5207253820357203\n",
      "\n",
      "\n",
      "3. Image URL: https://images-na.ssl-images-amazon.com/images/I/71dCrR30OvL._SY88.jpg\n",
      "Review: Looking at these on a guitar when they don't have a single wrap on the post, I found it hard to believe the strings would even stay on but I have now installed two sets of these on two different guitars. One black set and the other is gold. The first guitar would not stay in tune so I tried these. They have been on for nearly a year now and they totally fixed my tuning problem. The second set I put on my brand new Gibson Les Paul only because the first ones worked so well. Super easy fast string changes with the auto trim feature, and no tuning issues at all. At 18:1 gear ratio they dial in a perfect tune and hold it. I prefer these over Grover or any other higher priced tuners available.\n",
      "I have added a few pictures. The new tuners covered the holes on my Les Paul so it looks great from the back. The Other is an Ibanez AR420. You can see the original mounting holes so I filled them with wood putty and will use model car paint to match to cover the spots.\n",
      "Cosine score image: 0.8388147950172424\n",
      "Cosine score text: 0.19374981200602875\n",
      "Cosine score composite: 0.5162823035116356\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example input image URLs\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    magnitude_v1 = np.linalg.norm(v1)\n",
    "    magnitude_v2 = np.linalg.norm(v2)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if magnitude_v1 == 0 or magnitude_v2 == 0:\n",
    "        return 0\n",
    "\n",
    "    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n",
    "    return cosine_sim\n",
    "image_urls = []\n",
    "url_input = input(\"Enter image URL (or press Enter to finish): \").strip()\n",
    "review_input = input(\"REVIEW: \").strip()\n",
    "doc1 = preprocess_text(review_input)\n",
    "tf = [{word: doc1.count(word) / len(doc1) for word in doc1}]\n",
    "df = {}\n",
    "for doc in tokenized_texts:\n",
    "    for word in set(doc):\n",
    "        df[word] = df.get(word, 0) + 1\n",
    "\n",
    "# Calculate IDF (inverse document frequency)\n",
    "idf = {word: math.log(len(tokenized_texts) / freq) for word, freq in df.items()}\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf_doc1 = [{word: freq * idf[word] for word, freq in df.items()}]\n",
    "\n",
    "if url_input.startswith(\"[\") and url_input.endswith(\"]\"):\n",
    "    # Extract URLs from within square brackets\n",
    "    url_input = url_input[1:-1]\n",
    "    image_urls.extend([url.strip() for url in url_input.split(\",\")])\n",
    "elif url_input:\n",
    "    # If a single URL is provided without square brackets\n",
    "    image_urls.append(url_input)\n",
    "\n",
    "\n",
    "# Extract features from input images\n",
    "query_image_vectors = [extract_image_features_resnet(url) for url in image_urls]\n",
    "query_review_vector = pd.DataFrame(tf_idf_doc1)\n",
    "\n",
    "# Calculate similarities for images\n",
    "image_similarities = []\n",
    "for i, feature in enumerate(image_features_resnet):\n",
    "    similarities = []\n",
    "    for query_image_vector in query_image_vectors:\n",
    "        feature = feature.flatten()\n",
    "        cosine_sim_im = cosine_similarity(query_image_vector, feature)\n",
    "        similarities.append(cosine_sim_im)\n",
    "\n",
    "    cosine_sim_rv = cosine_similarity(query_review_vector.iloc[0], tf_idf_pd.iloc[i])\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    composite_similarity=(cosine_sim_rv+average_similarity)/2\n",
    "    image_similarities.append((i, average_similarity, cosine_sim_rv, composite_similarity))\n",
    "\n",
    "# Sort the list of similar images based on cosine similarity in descending order\n",
    "image_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "review_similarities = image_similarities.copy()\n",
    "composite_similarities= review_similarities.copy()\n",
    "\n",
    "review_similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "composite_similarities.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "# Get top 3 similar images\n",
    "top_3_similar_images = image_similarities[:3]\n",
    "top_3_similar_reviews = review_similarities[:3]\n",
    "top_3_similar_composites = composite_similarities[:3]\n",
    "\n",
    "# Save top 3 similar images\n",
    "with open('top_3_images.pkl', 'wb') as f:\n",
    "    pickle.dump(top_3_similar_images, f)\n",
    "\n",
    "with open('top_3_txt.pkl', 'wb') as f:\n",
    "    pickle.dump(top_3_similar_reviews, f)\n",
    "\n",
    "with open('top_3_composite.pkl', 'wb') as f:\n",
    "    pickle.dump(top_3_similar_composites, f)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Top 3 similar images:\")\n",
    "c1 = 1\n",
    "for i, img_similarity, txt_similarity, composite_similarity in top_3_similar_images:\n",
    "    print(f\"{c1}. Image URL: {cleaned_df['Image'][i]}\")\n",
    "    print(f\"Review: {cleaned_df['Review Text'][i]}\")\n",
    "    print(f\"Cosine score image: {img_similarity}\")\n",
    "    print(f\"Cosine score text: {txt_similarity}\")\n",
    "    print(f\"Cosine score composite: {composite_similarity}\")\n",
    "    print(\"\\n\")\n",
    "    c1 += 1\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Top 3 similar reviews:\")\n",
    "c2 = 1\n",
    "for i, img_similarity, txt_similarity ,composite_similarity in top_3_similar_reviews:\n",
    "    print(f\"{c2}. Image URL: {cleaned_df['Image'][i]}\")\n",
    "    print(f\"Review: {cleaned_df['Review Text'][i]}\")\n",
    "    print(f\"Cosine score image: {img_similarity}\")\n",
    "    print(f\"Cosine score text: {txt_similarity}\")\n",
    "    print(f\"Cosine score composite: {composite_similarity}\")\n",
    "    print(\"\\n\")\n",
    "    c2 += 1\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Top 3 similar composites:\")\n",
    "c3 = 1\n",
    "for i, img_similarity, txt_similarity, composite_similarity in top_3_similar_composites:\n",
    "    print(f\"{c3}. Image URL: {cleaned_df['Image'][i]}\")\n",
    "    print(f\"Review: {cleaned_df['Review Text'][i]}\")\n",
    "    print(f\"Cosine score image: {img_similarity}\")\n",
    "    print(f\"Cosine score text: {txt_similarity}\")\n",
    "    print(f\"Cosine score composite: {composite_similarity}\")\n",
    "    print(\"\\n\")\n",
    "    c3 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec942b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
